{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-sample t-tests\n",
    "\n",
    "Single-sample t-tests are used when you want to see if a list of values differs from an expected mean.\n",
    "\n",
    "Example: Do the favorability ratings for a political candidate differ from a neutral score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's say 5 is very favorable, and 1 is very unfavorable,\n",
    "# and 3 is neutral\n",
    "ratings = [2,3,4,2,3,5,5,1,3,4]\n",
    "\n",
    "np.mean(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average is higher than 3.0, but is it a statisically significant difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=0.48038446141526187, pvalue=0.642415137720789)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_1samp(ratings, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, the t-statistic is the ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error. (https://en.wikipedia.org/wiki/T-statistic)\n",
    "\n",
    "The p-value is the probability you'd obtain a similar result through chance alone. **If p-value is low, you have a statistically significant result.**\n",
    "\n",
    "In practice, p-value is the main number I look at. For casual use, I'd say this can be as high as 0.10, but for serious decision-making, you might want it to be 0.01 or even lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 2, 3, 5, 5, 1, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=2.697516588397716, pvalue=0.014265492922827512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What if we get 10 more 4's? Is it significant then?\n",
    "ratings += [4] * 10\n",
    "\n",
    "print(ratings)\n",
    "ttest_1samp(ratings, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might use this for something like [NPS scores](https://www.medallia.com/net-promoter-score/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related samples t-tests \n",
    "\n",
    "Related samples t-tests are a before-and-after test using the same people. The order of your list matters here.\n",
    "\n",
    "Example: Did people lose weight while on this drug? (Note that you'd still want a control group to compare against.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.6923076923076925, 4.153846153846154)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can imagine these are rat weights or something\n",
    "# I guess they're pretty fat rats\n",
    "before  = [2,3,4,5,3,2,5,6,7,8,9,3,4] \n",
    "after   = [1,2,3,4,5,6,7,2,4,3,6,7,4]\n",
    "\n",
    "# Order doesn't matter for mean, but it will when we do our t-test\n",
    "np.mean(before), np.mean(after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They weigh less *on average*, but is it sigificant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=0.6751399510385769, pvalue=0.5123861808484163)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_rel(before, after)\n",
    "# No meaningful difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.846153846153846, 4.153846153846154)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New comparison\n",
    "before = [2,3,4,5,3,2,9,6,7,8,9,9,9] \n",
    "after = [1,2,3,4,5,6,7,2,4,3,6,7,4]\n",
    "\n",
    "np.mean(before), np.mean(after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=2.381569860407206, pvalue=0.03466356528044936)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_rel(before, after)\n",
    "# Very likely a meaningful difference now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test is used a lot in science, but I'm having trouble thinking of business applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent sample t-tests\n",
    "\n",
    "Independent sample t-tests are used when you're comparing the scores of two different things.\n",
    "\n",
    "Example: Does this phone A *really* have a higher rating than phone B, or is our sample size too small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test doesn't require the arrays to be the same size. This is one reason why it's so much more powerful than just comparing the averages; the output will indirectly tell you whether your sample size is too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0588235294117645, 3.4285714285714284)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [2,3,5,2,3,4,2,2,4,2,1,3,4,5,3,3,4]\n",
    "b = [2,3,3,3,4,4,4,4,3,3,3,4,4,4]\n",
    "\n",
    "np.mean(a), np.mean(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.074383566184422, pvalue=0.29150920254640794)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Not significant\n",
    "ttest_ind(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-square / A-B Tests\n",
    "\n",
    "I prefer to juse use an online calculator for this. Here's my favorite:\n",
    "\n",
    "https://abtestguide.com/calc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When should you use a one vs. two-tailed test?\n",
    "\n",
    "https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-what-are-the-differences-between-one-tailed-and-two-tailed-tests/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
